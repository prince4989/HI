{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**APPENDIX**"
      ],
      "metadata": {
        "id": "NfuPK8_K9XU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y8FI2jBEUmR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCc-SWgeNQlq"
      },
      "outputs": [],
      "source": [
        "import os  \n",
        "import re  \n",
        "import ast  \n",
        "import json     \n",
        "import glob \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FirqonloNxQp"
      },
      "outputs": [],
      "source": [
        "DATA_PATH=\"/content/drive/MyDrive/nbme-score-clinical-patient-notes/\"\n",
        "OUT_PATH=\"/content/drive/MyDrive/roberta-large/\"\n",
        "WEIGHTS_FOLDER=\"/content/drive/MyDrive/roberta-large/\"\n",
        "NUM_WORKERS=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTvRGwpP5nDj"
      },
      "outputs": [],
      "source": [
        "def process_feature_text(text):\n",
        "  text=re.sub('I-year','1-year',text) \n",
        "  text=re.sub('-OR-',\" or \",text)\n",
        "  text=re.sub('-',' ',text)\n",
        "  return text\n",
        "\n",
        "def clean_spaces(txt):\n",
        "  txt = re.sub('\\n',' ', txt)\n",
        "  txt = re.sub('\\t',' ', txt)\n",
        "  txt = re.sub('\\r',' ', txt)\n",
        "  return txt\n",
        "\n",
        "\n",
        "def load_and_prepare_test(root=\"\"):\n",
        "    patient_notes=pd.read_csv(root+\"patient_notes.csv\")\n",
        "    features=pd.read_csv(root+\"features.csv\")\n",
        "    df=pd.read_csv(root+\"test.csv\")\n",
        "    df=df.merge (features,how=\"left\",on=[\"case_num\",\"feature_num\"])\n",
        "    df=df.merge(patient_notes,how=\"left\",on=['case_num','pn_num'])\n",
        "    df['pn_history'] = df['pn_history'].apply (lambda x: x.strip()) \n",
        "    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n",
        "    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n",
        "    df['clean_text'] = df['pn_history'].apply(clean_spaces)\n",
        "    df['target']=\"\"\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaG-zXKD5pVb"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "def token_pred_to_char_pred(token_pred, offsets):\n",
        "    char_pred=np.zeros((np.max(offsets), token_pred.shape[1]))\n",
        "    for i in range(len(token_pred)):\n",
        "        s,e=int(offsets[i][0]),int(offsets[i][1])\n",
        "        char_pred[s:e] = token_pred[i]\n",
        "\n",
        "        if token_pred.shape[1]==3:\n",
        "            s=s+1\n",
        "            char_pred[s: e,1], char_pred[s: e,2]=(np.max(char_pred[s: e,1:],1),np.min(char_pred[s: e,1:],1),)\n",
        "    return char_pred\n",
        "\n",
        "def labels_to_Sub(labels):\n",
        "    all_spans=[]\n",
        "    for label in labels:\n",
        "        indices=np.where(label>0)[0]\n",
        "        indices_grouped=[\n",
        "            list(g) for _, g in itertools.groupby(\n",
        "                indices, key=lambda n, c=itertools.count(): n - next(c))]\n",
        "\n",
        "        spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n",
        "        all_spans.append(\";\".join(spans))\n",
        "    return all_spans\n",
        "\n",
        "\n",
        "def char_target_to_span(char_target): \n",
        "    spans=[]\n",
        "    start,end=0,0\n",
        "    for i in range(len(char_target)):\n",
        "        if char_target[i]==1 and char_target[i-1]==0:\n",
        "            if end:\n",
        "              spans.append([start, end])\n",
        "            start=i\n",
        "            end=i+1\n",
        "        elif char_target[i]==1:\n",
        "            end=i+1\n",
        "        else:\n",
        "            if end:\n",
        "                spans.append([start,end])\n",
        "            start,end=0,0\n",
        "    return spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usa9qK9yC_12"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjKhQQPS57eW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def get_tokenizer(name, precompute=False, df=None, folder=None):\n",
        "  if folder is None: \n",
        "        tokenizer=AutoTokenizer.from_pretrained(name)\n",
        "  else:\n",
        "        tokenizer=AutoTokenizer.from_pretrained(folder)\n",
        "\n",
        "  tokenizer.name=name\n",
        "  tokenizer.special_tokens={\n",
        "        \"sep\": tokenizer.sep_token_id,\n",
        "        \"cls\": tokenizer.cls_token_id,\n",
        "        \"pad\": tokenizer.pad_token_id,\n",
        "    }\n",
        "\n",
        "  if precompute:\n",
        "        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n",
        "  else:\n",
        "        tokenizer.precomputed = None\n",
        "\n",
        "  return tokenizer\n",
        "\n",
        "\n",
        "def precompute_tokens(df, tokenizer):\n",
        "    feature_texts = df[\"feature_text\"].unique()\n",
        "    ids={}\n",
        "    offsets={}\n",
        "\n",
        "    for feature_text in feature_texts:\n",
        "        encoding = tokenizer(\n",
        "            feature_text,\n",
        "            return_token_type_ids=True,\n",
        "            return_offsets_mapping=True,\n",
        "            return_attention_mask=False,\n",
        "            add_special_tokens=False,)\n",
        "        ids[feature_text]=encoding[\"input_ids\"]\n",
        "        offsets[feature_text]=encoding[\"offset_mapping\"]\n",
        "\n",
        "    texts = df[\"clean_text\"].unique()\n",
        "    for text in texts:\n",
        "        encoding=tokenizer(\n",
        "            text,\n",
        "            return_token_type_ids=True,\n",
        "            return_offsets_mapping=True,\n",
        "            return_attention_mask=False,\n",
        "            add_special_tokens=False,)\n",
        "        ids[text] = encoding[\"input_ids\"]\n",
        "        offsets[text] = encoding[\"offset_mapping\"]\n",
        "\n",
        "    return {\"ids\": ids, \"offsets\": offsets}\n",
        "\n",
        "\n",
        "def encodings_from_precomputed(feature_text, text, precomputed, tokenizer, max_len=300):\n",
        "    tokens = tokenizer.special_tokens\n",
        "\n",
        "    if \"roberta\" in tokenizer.name:\n",
        "        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n",
        "    else:\n",
        "        qa_sep = [tokens[\"sep\"]]\n",
        "\n",
        "    input_ids=[tokens[\"cls\"]]+precomputed[\"ids\"][feature_text]+qa_sep\n",
        "    n_question_tokens=len(input_ids)\n",
        "\n",
        "    input_ids=input_ids+precomputed[\"ids\"][text]\n",
        "    input_ids=input_ids[: max_len - 1] + [tokens[\"sep\"]]\n",
        "\n",
        "    if \"roberta\" not in tokenizer.name:\n",
        "        token_type_ids = np.ones(len(input_ids))\n",
        "        token_type_ids[:n_question_tokens] = 0\n",
        "        token_type_ids = token_type_ids.tolist()\n",
        "    else:\n",
        "        token_type_ids=[0]*len(input_ids)\n",
        "\n",
        "    offsets=[(0, 0)]*n_question_tokens+precomputed[\"offsets\"][text]\n",
        "    offsets=offsets[: max_len - 1]+[(0, 0)]\n",
        "\n",
        "    padding_length=max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        offsets = offsets + ([(0, 0)] * padding_length)\n",
        "\n",
        "    encoding = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"token_type_ids\": token_type_ids,\n",
        "        \"offset_mapping\": offsets,  }\n",
        "\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb7yDIwMCRId"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "class PatientNoteDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.df=df\n",
        "        self.max_len=max_len\n",
        "        self.tokenizer=tokenizer\n",
        "        self.texts=df['clean_text'].values\n",
        "        self.feature_text=df['feature_text'].values\n",
        "        self.char_targets=df['target'].values.tolist()\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        text=self.texts[idx]\n",
        "        feature_text=self.feature_text[idx]\n",
        "        char_target=self.char_targets[idx]\n",
        "        if self.tokenizer.precomputed is None:\n",
        "            encoding = self.tokenizer(\n",
        "                feature_text,\n",
        "                text,\n",
        "                return_token_type_ids=True,\n",
        "                return_offsets_mapping=True,\n",
        "                return_attention_mask=False,\n",
        "                truncation=\"only_second\",\n",
        "                max_length=self.max_len,\n",
        "                padding='max_length',\n",
        "            )\n",
        "            raise NotImplementedError(\"fix issues with question offsets\")\n",
        "        else:\n",
        "            encoding = encodings_from_precomputed(\n",
        "                feature_text,\n",
        "                text,\n",
        "                self.tokenizer.precomputed,\n",
        "                self.tokenizer,\n",
        "                max_len=self.max_len\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n",
        "            \"target\": torch.tensor([0], dtype=torch.float),\n",
        "            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n",
        "            \"text\": text,\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt9qp81-DMZX"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "def plot_annotation(df, pn_num):\n",
        "    options = {\"colors\": {}}\n",
        "\n",
        "    df_text = df[df[\"pn_num\"] == pn_num].reset_index(drop=True)\n",
        "\n",
        "    text = df_text[\"pn_history\"][0]\n",
        "    ents = []\n",
        "\n",
        "    for spans, feature_text, feature_num in df_text[[\"span\", \"feature_text\", \"feature_num\"]].values:\n",
        "        for s in spans:\n",
        "            ents.append({\"start\": int(s[0]), \"end\": int(s[1]), \"label\": feature_text})\n",
        "\n",
        "        options[\"colors\"][feature_text] =  f\"rgb{tuple(np.random.randint(100, 255, size=3))}\"\n",
        "\n",
        "    doc = {\"text\": text, \"ents\": sorted(ents, key=lambda i: i[\"start\"])}\n",
        "    spacy.displacy.render(doc, style=\"ent\", options=options, manual=True, jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0OHoMMVDQEn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from transformers import AutoConfig, AutoModel\n",
        "\n",
        "class NERTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        num_classes=1,\n",
        "        config_file=None,\n",
        "        pretrained=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.name = model\n",
        "        self.pad_idx = 1 if \"roberta\" in self.name else 0\n",
        "\n",
        "        transformers.logging.set_verbosity_error()\n",
        "\n",
        "        if config_file is None:\n",
        "            config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
        "        else:\n",
        "            config = torch.load(config_file)\n",
        "\n",
        "        if pretrained:\n",
        "            self.transformer = AutoModel.from_pretrained(model, config=config)\n",
        "        else:\n",
        "            self.transformer = AutoModel.from_config(config)\n",
        "\n",
        "        self.nb_features = config.hidden_size\n",
        "        self.logits = nn.Linear(self.nb_features, num_classes)\n",
        "\n",
        "    def forward(self, tokens, token_type_ids):\n",
        "        hidden_states = self.transformer(\n",
        "            tokens,\n",
        "            attention_mask=(tokens != self.pad_idx).long(),\n",
        "            token_type_ids=token_type_ids,\n",
        "        )[-1]\n",
        "        features = hidden_states[-1]\n",
        "        logits = self.logits(features)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDUn1w7QDQuZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def load_model_weights(model, filename, verbose=1, cp_folder=\"\", strict=True):\n",
        "    if verbose:\n",
        "        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(\n",
        "            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n",
        "            strict=strict,\n",
        "        )\n",
        "    except RuntimeError:\n",
        "        model.encoder.fc = torch.nn.Linear(model.nb_ft, 1)\n",
        "        model.load_state_dict(\n",
        "            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n",
        "            strict=strict,\n",
        "        )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo_Ilr6tDfjd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "def predict(model, dataset, data_config, activation=\"softmax\"):\n",
        "    model.eval()\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=data_config['val_bs'],\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(loader):\n",
        "            ids, token_type_ids = data[\"ids\"], data[\"token_type_ids\"]\n",
        "\n",
        "            y_pred = model(ids.cuda(), token_type_ids.cuda())\n",
        "\n",
        "            if activation == \"sigmoid\":\n",
        "                y_pred = y_pred.sigmoid()\n",
        "            elif activation == \"softmax\":\n",
        "                y_pred = y_pred.softmax(-1)\n",
        "\n",
        "            preds += [token_pred_to_char_pred(y, offsets) for y, offsets\n",
        "                in zip(y_pred.detach().cpu().numpy(), data[\"offsets\"].numpy())]\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOoRXnv5DgCI"
      },
      "outputs": [],
      "source": [
        "def inference_test(df, exp_folder, config, cfg_folder=None):\n",
        "    preds = []\n",
        "\n",
        "    if cfg_folder is not None:\n",
        "        model_config_file = cfg_folder + config.name.split('/')[-1] + \"config.pth\"\n",
        "        print(model_config_file)\n",
        "        tokenizer_folder = cfg_folder + config.name.split('/')[-1] + \"/tokenizers/\"\n",
        "    else:\n",
        "        model_config_file, tokenizer_folder = None, None\n",
        "\n",
        "    tokenizer = get_tokenizer(\n",
        "        config.name, precompute=config.precompute_tokens, df=df, folder=tokenizer_folder\n",
        "    )\n",
        "\n",
        "    dataset=PatientNoteDataset(df,\n",
        "        tokenizer,\n",
        "        max_len=config.max_len,)\n",
        "\n",
        "    model=NERTransformer(\n",
        "        config.name,\n",
        "        num_classes=config.num_classes,\n",
        "        config_file=model_config_file,\n",
        "        pretrained=False).cuda()\n",
        "    model.zero_grad()\n",
        "\n",
        "    weights=sorted(glob.glob(exp_folder+\"*.pt\"))\n",
        "    for weight in weights:\n",
        "        model=load_model_weights(model,weight)\n",
        "\n",
        "        pred=predict(\n",
        "            model,\n",
        "            dataset,\n",
        "            data_config=config.data_config,\n",
        "            activation=config.loss_config[\"activation\"]\n",
        "        )\n",
        "        preds.append(pred)\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj9slTZqDkbe"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    name=\"/content/drive/MyDrive/\"\n",
        "    num_classes=1\n",
        "\n",
        "    max_len = 310\n",
        "    precompute_tokens=True\n",
        "\n",
        "    loss_config = {\"activation\": \"sigmoid\",}\n",
        "\n",
        "    data_config = {\n",
        "        \"val_bs\": 16 if \"large\" in name else 32,\n",
        "        \"pad_token\": 1 if \"roberta\" in name else 0,\n",
        "    }\n",
        "    verbose = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTY0wC8TEGEJ"
      },
      "outputs": [],
      "source": [
        "df_test=load_and_prepare_test(root=DATA_PATH)\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "J11e7BieFl2o",
        "outputId": "ec394afb-524f-457b-8eb7-dfd5e5ece249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/roberta-large/config.pth\n"
          ]
        }
      ],
      "source": [
        "preds=inference_test(\n",
        "    df_test,\n",
        "    WEIGHTS_FOLDER,\n",
        "    Config,\n",
        "    cfg_folder=OUT_PATH)\n",
        "[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3rAHtE6Do_y"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    df_test['span'] = df_test['preds'].apply(char_target_to_span)\n",
        "    plot_annotation(df_test, df_test['pn_num'][0])\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR6gny2xDr_w"
      },
      "outputs": [],
      "source": [
        "def post_process_spaces(target, text):\n",
        "    target = np.copy(target)\n",
        "    if len(text) > len(target):\n",
        "        padding = np.zeros(len(text) - len(target))\n",
        "        target = np.concatenate([target, padding])\n",
        "    else:\n",
        "        target = target[:len(text)]\n",
        "\n",
        "    if text[0] == \" \":\n",
        "        target[0] = 0\n",
        "    if text[-1] == \" \":\n",
        "        target[-1] = 0\n",
        "\n",
        "    for i in range(1, len(text) - 1):\n",
        "        if text[i] == \" \":\n",
        "            if target[i] and not target[i - 1]:  # space before\n",
        "                target[i] = 0\n",
        "\n",
        "            if target[i] and not target[i + 1]:  # space after\n",
        "                target[i] = 0\n",
        "\n",
        "            if target[i - 1] and target[i + 1]:\n",
        "                target[i] = 1\n",
        "\n",
        "    return target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMfplF8zDvN0"
      },
      "outputs": [],
      "source": [
        "df_test['preds_pp'] = df_test.apply(lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMafsBI4Dw7-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DeepLearning_Project_NBME",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}